import numpy as np
from sklearn.model_selection import KFold
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import PolynomialFeatures
from sklearn.datasets import fetch_california_housing

def polynomial_regression(degree, X, y, folds, test_size=0.25, random_state=None):
    # Define number of folds for cross-validation
    kf = KFold(folds)

    # Initialize lists to store results for variance, bias2s, total_error, and models
    variances = []
    bias2s = []
    total_errors = []
    models = []

    # Set the polynomial degree of the model
    poly_features = PolynomialFeatures(degree)
    X_poly = poly_features.fit_transform(X)

    # Perform cross-validation
    for train_index, test_index in kf.split(X_poly):
        # Split data into training and testing sets for this fold
        X_train, X_test = X_poly[train_index], X_poly[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Fit polynomial regression model
        model = LinearRegression()
        model.fit(X_train, y_train)

        # Make predictions on the test set
        y_pred = model.predict(X_test)

        # Calculate variance and bias for this fold
        variance = np.var(y_pred)
        bias2 = np.mean((y_test - y_pred)**2)
        
        # Append results to lists
        variances.append(variance)
        bias2s.append(bias2)
        total_errors.append(variance + bias2)
        models.append(model)

        # Print results for this fold
        print("Variance: {:.4f}, Bias2: {:.4f}, Total error: {:.4f}".format(variance, bias2, bias2 + variance))

    # print the total_error of the best model
    min_error_index = np.argmin(total_errors)
    best_model = models[min_error_index]
    print(f"Best model total error: {total_errors[min_error_index]:.4f}")

    # Testing the final model on the test data
    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=test_size, random_state=42)

    # Obtain the predictions on the test data
    y_pred = best_model.predict(X_test)
    
    # store mse score of the model applied on the test data
    mse = mean_squared_error(y_test, y_pred)
    
    return mse , best_model



# Example usage: load California Housing Dataset and select the first, third, and forth attributes as input features in X
housing = fetch_california_housing()
# Set the target valiable 
X = housing.data[:, [0, 2, 3]]
y = housing.target

degrees = range(1, 6)  # Try polynomial degrees from 1 to 5
# Try degrees from 1 to 5 and in a loop, report mse of the best model trained using k-fold cross validation and print("Degree:", degree, "MSE:", mse)
for degree in degrees:
    print(f"\nDegree: {degree}")
    mse, best_model = polynomial_regression(degree, X, y, folds=5)
    print(f"MSE: {mse:.4f}")
